import json
import os
import re
import pickle
import math
from collections import defaultdict
from pathlib import Path
from datetime import datetime, date, timedelta
import numpy as np
import pandas as pd
from datasets import Dataset
from ragas import evaluate
# â˜… 1. context_relevancyë¥¼ import ëª©ë¡ì—ì„œ ì œì™¸
from ragas.metrics import (
    faithfulness,
    answer_relevancy
)
from tqdm import tqdm

# --- LangChain ë° ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

try:
    # langchain_communityê°€ ìš°ì„ 
    from langchain_community.retrievers import BM25Retriever
except Exception:
    from langchain.retrievers import BM25Retriever  # fallback

# (ì˜µì…˜) í¬ë¡œìŠ¤ ì¸ì½”ë” â€” ì—†ìœ¼ë©´ ìë™ í´ë°±
try:
    from sentence_transformers import CrossEncoder
except Exception:
    CrossEncoder = None
    print("[Warning] sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Rerankerê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

import openai

# --- ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
load_dotenv()

# LLM & ì„ë² ë”© (rag.pyì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
llm = ChatOpenAI(model="gpt-4o-mini")
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
persist_directory = str(BASE_DIR / "news_chroma_db")
bm25_index_path = BASE_DIR / "bm25_index.pkl"

# ì„¤ì • íŒŒì¼ ë¡œë“œ (config.json)
try:
    with open(BASE_DIR / "config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
except Exception:
    config = {}

# íŒ€ ì´ë¦„ ë³€í™˜ ì‚¬ì „
translation_dict = config.get("translation_dict", {})

# ì „ì—­ ë¦¬íŠ¸ë¦¬ë²„ ë° ë­ì»¤ ë³€ìˆ˜
vector_retriever = None
bm25_global = None
bm25_doc_count = 0
bm25_all_docs = []
reranker = None
parser = StrOutputParser()


# --- 1. rag.pyì˜ í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (ë³µì‚¬) ---
# ì´ í•¨ìˆ˜ë“¤ì€ test.pyê°€ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.

def translate_query(query: str, dictionary: dict) -> str:
    """ê°„ë‹¨í•œ ë‹¨ì–´ ê²½ê³„ ì¹˜í™˜(ì‚¬ì „ ì—†ìœ¼ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ)"""
    if not query or not dictionary:
        return query
    for kor, eng in dictionary.items():
        query = query.replace(kor, eng)
    return query

def rrf_fuse(result_lists, k=36, C=60):
    """Reciprocal Rank Fusion: ì—¬ëŸ¬ ë¦¬ìŠ¤íŠ¸ì˜ ë“±ìˆ˜ë¥¼ í•©ì‚°í•´ ìƒìœ„ kê°œ ì„ íƒ"""
    scores, pick = defaultdict(float), {}
    for results in result_lists: 
        for rank, d in enumerate(results):
            key = d.page_content
            scores[key] += 1.0 / (C + rank + 1)
            pick.setdefault(key, d)
    merged = [pick[key] for key, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)]
    return merged[:k]

def gpt_translate_korean_to_english(query: str, model="gpt-4o-mini") -> str:
    """GPTë¥¼ ì´ìš©í•œ í•œ->ì˜ ë²ˆì—­"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Translate the following Korean football question into English for use in a document search engine. Be concise."),
        ("human", "{q}")
    ])
    chain = prompt | ChatOpenAI(model=model, temperature=0) | StrOutputParser()
    return chain.invoke({"q": query})

def build_global_bm25():
    """
    (rag.pyì™€ ë™ì¼) ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ ë° DBì™€ ë¹„êµí•˜ì—¬ ì—…ë°ì´íŠ¸
    """
    global bm25_global, bm25_doc_count, bm25_all_docs

    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=hf_embeddings,
        collection_name="news_collection",
    )

    # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ
    old_docs = []
    processed_ids = set()
    if bm25_index_path.exists():
        try:
            with open(bm25_index_path, "rb") as f:
                saved_data = pickle.load(f)
                old_docs = saved_data.get('docs', [])
                processed_ids = saved_data.get('ids', {d.metadata.get('id') for d in old_docs})
            print(f"[bm25] {len(processed_ids)}ê°œì˜ ê¸°ì¡´ ë¬¸ì„œ ì •ë³´ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"[bm25] ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            old_docs = []
            processed_ids = set()

    # 2. ChromaDBì—ì„œ í˜„ì¬ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    try:
        all_db_ids = set(db.get(include=[])['ids'])
        if not all_db_ids:
            print("[bm25] DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
    except Exception as e:
        print(f"[bm25] DBì—ì„œ IDë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return len(old_docs)

    # 3. ìƒˆë¡œìš´ ë¬¸ì„œ ID ì°¾ê¸°
    new_doc_ids = list(all_db_ids - processed_ids)

    # 4. ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©
    if not new_doc_ids:
        print("[bm25] ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        if bm25_global is None and old_docs:
            bm25_global = BM25Retriever.from_documents(old_docs)
            bm25_global.k = int(config.get("bm25_k", 20))
            bm25_all_docs = old_docs
            bm25_doc_count = len(old_docs)
        return len(processed_ids)

    # 5. ìƒˆë¡œìš´ ë¬¸ì„œë§Œ DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
    print(f"[bm25] {len(new_doc_ids)}ê°œì˜ ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    new_docs_data = db.get(ids=new_doc_ids, include=["documents", "metadatas"])
    new_docs = [
        Document(page_content=c, metadata=m)
        for c, m in zip(new_docs_data.get("documents", []), new_docs_data.get("metadatas", []))
        if c
    ]
    
    # 6. ê¸°ì¡´ + ì‹ ê·œ ë¬¸ì„œë¡œ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    final_docs = old_docs + new_docs
    print(f"[bm25] ì´ {len(final_docs)}ê°œ ë¬¸ì„œë¡œ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
    bm25 = BM25Retriever.from_documents(final_docs)
    bm25.k = int(config.get("bm25_k", 20))
    
    # 7. ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ ë° íŒŒì¼ ì €ì¥
    bm25_global = bm25
    bm25_all_docs = final_docs
    bm25_doc_count = len(final_docs)

    try:
        with open(bm25_index_path, "wb") as f:
            pickle.dump({'docs': final_docs, 'ids': all_db_ids}, f)
        print(f"[bm25] ìµœì‹  ì¸ë±ìŠ¤ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {bm25_index_path}")
    except Exception as e:
        print(f"[bm25] ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    return bm25_doc_count

def init_reranker_from_config(cfg: dict):
    """í¬ë¡œìŠ¤ ì¸ì½”ë” ì´ˆê¸°í™” (rag.pyì™€ ë™ì¼)"""
    global reranker
    if not cfg.get("use_reranker", True):
        return
    if CrossEncoder is None:
        print("[reranker] sentence-transformers ë¯¸ì„¤ì¹˜ â†’ ê±´ë„ˆëœ€")
        return
    model = cfg.get("reranker_model", "BAAI/bge-reranker-base")
    max_len = int(cfg.get("reranker_max_length", 512))
    try:
        reranker = CrossEncoder(model, max_length=max_len)
        print(f"[reranker] loaded: {model}")
    except Exception as e:
        reranker = None
        print(f"[reranker] load failed: {e}")

def rerank_with_cross_encoder(query: str, docs, top_n=12, batch_size=16):
    """í¬ë¡œìŠ¤ ì¸ì½”ë” ì¬ë­í¬ (rag.pyì™€ ë™ì¼)"""
    if not docs:
        return []
    if reranker is None:
        return docs[:top_n]
    pairs = [[query, d.page_content] for d in docs]
    try:
        scores = reranker.predict(pairs, batch_size=batch_size, show_progress_bar=False)
    except Exception:
        return docs[:top_n]
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, _ in ranked[:top_n]]

def _detect_news_category(q: str) -> str:
    """(rag.pyì™€ ë™ì¼) GPTë¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    prompt = f"""
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¶•êµ¬ 'ë‰´ìŠ¤' ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë‹¤ìŒ 6ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”.

        - transfer: ì´ì ì„¤, ì˜ì…, ë°©ì¶œ, ì¬ê³„ì•½ ê´€ë ¨ ì§ˆë¬¸
        - injury: ì„ ìˆ˜ì˜ ë¶€ìƒ, ì§•ê³„, ì»¨ë””ì…˜ ë¬¸ì œ ê´€ë ¨ ì§ˆë¬¸
        - preview: ì•ìœ¼ë¡œ ì—´ë¦´ ê²½ê¸°ì— ëŒ€í•œ ì˜ˆì¸¡, ê´€ì „ í¬ì¸íŠ¸, ì˜ˆìƒ ë¼ì¸ì—… ê´€ë ¨ ì§ˆë¬¸
        - review: ì´ë¯¸ ëë‚œ ê²½ê¸°ì˜ ê²°ê³¼, í•˜ì´ë¼ì´íŠ¸, ë¶„ì„, ê²°ì •ì  ì¥ë©´ ê´€ë ¨ ì§ˆë¬¸
        - performance: íŠ¹ì • ì„ ìˆ˜ë‚˜ íŒ€ì˜ ìµœê·¼ í™œì•½ìƒ, í¼, ìŠ¤íƒ¯, í‰ê°€ ê´€ë ¨ ì§ˆë¬¸
        - general: ìœ„ì˜ 5ê°€ì§€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ëª¨ë“  ì¼ë°˜ì ì¸ ì •ë³´ ì§ˆë¬¸

        **ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì•„ë˜ 6ê°œì˜ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.**
        transfer / injury / preview / review / performance / general

        ì‚¬ìš©ì ì§ˆë¬¸: "{q}"
        ë¶„ë¥˜:
        """
    try:
        # v1.x openai êµ¬ë¬¸ ì‚¬ìš© (rag.pyì˜ ì½”ë“œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ìµœì‹  ë²„ì „ ê¸°ì¤€)
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        result = response.choices[0].message.content.strip().lower()
        
        valid_categories = {"transfer", "injury", "preview", "review", "performance", "general"}
        return result if result in valid_categories else "general"
    except Exception as e:
        print(f"ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "general"

def _build_chain_for_news(category: str):
    """(rag.pyì™€ ë™ì¼) ì¹´í…Œê³ ë¦¬ë³„ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    system_message = ""
    # (ì¹´í…Œê³ ë¦¬ë³„ system_message ì •ì˜ - rag.pyì—ì„œ ë³µì‚¬)
    if category == "transfer":
        system_message = (
            "ë‹¹ì‹ ì€ ì´ì  ì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, "
            "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ì´ì ì„¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤. "
            "íŠ¹íˆ **'ì„ ìˆ˜ ì´ë¦„', 'ê´€ë ¨ êµ¬ë‹¨', 'ì˜ˆìƒ ì´ì ë£Œ/ì¡°ê±´', 'ë£¨ë¨¸ì˜ ì¶œì²˜ë‚˜ ì‹ ë¢°ë„'**ì— ì´ˆì ì„ ë§ì¶° ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì„¸ìš”. "
            "ì¶”ì¸¡ì€ ìµœì†Œí™”í•˜ê³ , ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë³¸ë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ë¡ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤."
        )
    elif category == "injury":
        system_message = (
            "ë‹¹ì‹ ì€ êµ¬ë‹¨ì˜ ê³µì‹ ì˜ë£ŒíŒ€ì²˜ëŸ¼ ë³´ê³ í•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ë“¤ì„ ê·¼ê±°ë¡œ, "
            "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ì„ ìˆ˜ì˜ ìƒíƒœì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë³¸ë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ë¡ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤."
            "**'ì„ ìˆ˜ ì´ë¦„', 'ë¶€ìƒ ë¶€ìœ„ ë° ì‹¬ê°ë„', 'ì˜ˆìƒ ê²°ì¥ ê¸°ê°„ ë˜ëŠ” ë³µê·€ ì‹œì '**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
        )
    # ... (preview, review, performance ì¹´í…Œê³ ë¦¬ ë©”ì‹œì§€ - ìƒëµë˜ì—ˆì§€ë§Œ ë™ì¼í•˜ê²Œ ë³µì‚¬) ...
    else: # general ë° ê¸°íƒ€
        system_message = (
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¶•êµ¬ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤. "
            "ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì°¾ì•„ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. "
            "í•­ìƒ ê°ê´€ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ì •ë³´ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤."
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë³¸ë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ë¡ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤."
        )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "ì•„ë˜ëŠ” ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° í•„ìš”í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n---\n{context}\n---\n\nì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n{input}")
    ])
    
    # llmê³¼ parserëŠ” ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
    return prompt | llm | parser

def create_rag_chain():
    """
    (rag.pyì™€ ë™ì¼)
    í‰ê°€ì— í•„ìš”í•œ ëª¨ë“  ì „ì—­ ë¦¬íŠ¸ë¦¬ë²„ (Vector, BM25, Reranker)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    global vector_retriever, bm25_global, reranker, config

    # 1. ë²¡í„° DB ë¡œë“œ
    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=hf_embeddings,
        collection_name="news_collection",
    )

    # 2. ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    if config.get("use_mmr", True):
        vector_retriever = db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": int(config.get("mmr_k", 20)),
                "fetch_k": int(config.get("mmr_fetch_k", 80)),
                "lambda_mult": float(config.get("mmr_lambda", 0.7)),
            },
        )
    else:
        vector_retriever = db.as_retriever(search_kwargs={"k": int(config.get("k", 20))})

    # 3. BM25 êµ¬ì¶•/ì´ˆê¸°í™”
    build_global_bm25()

    # 4. (ì˜µì…˜) ì¬ë­ì»¤ ì¤€ë¹„
    init_reranker_from_config(config)

    # 5. ìƒíƒœ ë©”ì‹œì§€ ë°˜í™˜
    try:
        count = db._collection.count()
    except Exception:
        count = 0
    return f"ì¤€ë¹„ ì™„ë£Œ / DB ë¬¸ì„œ ìˆ˜: {count} / BM25 ë¬¸ì„œ ìˆ˜: {bm25_doc_count}"


# --- 2. RAGAs í‰ê°€ ë¡œì§ (ê¸°ì¡´ test.py ì½”ë“œ) ---

def load_testset(filepath="ragas_dataset.jsonl"):
    """'satisfied' í”¼ë“œë°±('reason'ì´ ì—†ëŠ”) ì§ˆë¬¸ë§Œ ë¡œë“œ"""
    questions = []
    
    dataset_path = BASE_DIR / filepath
    if not dataset_path.exists():
        print(f"ì˜¤ë¥˜: {dataset_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    with open(dataset_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                if "reason" not in data and data.get("question"):
                    questions.append(data["question"])
            except json.JSONDecodeError:
                continue
    
    # ì¤‘ë³µ ì œê±° í›„ 30ê°œ ìƒ˜í”Œë§
    unique_questions = list(set(questions))
    print(f"'{filepath}'ì—ì„œ {len(unique_questions)}ê°œì˜ ê³ ìœ í•œ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    # â˜…â˜…â˜… ë…¼ë¬¸ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸°ë¥¼ 30ê°œë¡œ ê³ ì • (ë°ì´í„°ê°€ ì ìœ¼ë©´ ê·¸ ì´í•˜)
    sample_size = min(len(unique_questions), 30)
    return unique_questions[:sample_size]

def get_rag_results(question: str, model_type: str):
    """
    ì§€ì •ëœ model_typeì— ë”°ë¼ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰í•˜ê³ ,
    RAGAs í‰ê°€ì— í•„ìš”í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # 1. ì§ˆì˜ ì „ì²˜ë¦¬
    q_preprocessed = translate_query(question, translation_dict).lower()
    q_translated = gpt_translate_korean_to_english(q_preprocessed)
    # 2. ë¦¬íŠ¸ë¦¬ë²Œ (í•­ìƒ ë‘˜ ë‹¤ ì‹¤í–‰)
    vector_docs = vector_retriever.invoke(q_translated)
    bm_docs = bm25_global.invoke(q_translated) if bm25_global is not None else []
    
    # 3. ëª¨ë¸ íƒ€ì…ë³„ í›„ë³´êµ°(final_docs) í™•ì •
    final_docs = []
    
    # RAG ë…¼ë¬¸ì—ì„œ Kê°’(ê²€ìƒ‰ ê°œìˆ˜)ì„ í†µì¼í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    TOP_K = int(config.get("rrf_k", 10))
    
    if model_type == "baseline_vector":
        final_docs = vector_docs[:TOP_K]
    
    elif model_type == "baseline_bm25":
        final_docs = bm_docs[:TOP_K]
        
    elif model_type == "hybrid_rrf":
        candidates = rrf_fuse(
            [vector_docs, bm_docs],
            k=int(config.get("rrf_candidates_k", 20)),
            C=int(config.get("rrf_C", 60)),
        )
        final_docs = candidates[:TOP_K] # Reranker ì—†ì´ ìƒìœ„ Kê°œ
        
    elif model_type == "final_rrf_rerank":
        candidates = rrf_fuse(
            [vector_docs, bm_docs],
            k=int(config.get("rrf_candidates_k", 20)),
            C=int(config.get("rrf_C", 60)),
        )
        final_docs = rerank_with_cross_encoder(
            question, # ì¬ë­ì»¤ëŠ” ì›ë³¸ í•œê¸€ ì§ˆë¬¸ ì‚¬ìš©
            candidates,
            top_n=TOP_K,
            batch_size=int(config.get("reranker_batch_size", 16)),
        )

    # 4. ë‹µë³€ ìƒì„±
    context_str = "\n\n".join(d.page_content for d in final_docs)
    
    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° ë™ì  í”„ë¡¬í”„íŠ¸ ì²´ì¸ ì‚¬ìš©
    category = _detect_news_category(question)
    rag_chain = _build_chain_for_news(category)
    
    try:
        answer = rag_chain.invoke({"context": context_str, "input": question})
    except Exception as e:
        print(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜ (ì§ˆë¬¸: {question[:20]}...): {e}")
        answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    contexts = [d.page_content for d in final_docs]
    
    return {
        "question": question,
        "answer": answer,
        "contexts": contexts
    }

def run_evaluation():
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""
    
    # 0. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Vector, BM25, Reranker ë¡œë“œ)
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    init_status = create_rag_chain()
    print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {init_status}")

    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    questions = load_testset("ragas_dataset.jsonl")
    if not questions:
        print("í‰ê°€í•  ì§ˆë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. RAGAS_DATASET_FILEì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ RAGAs í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    models_to_test = [
        "baseline_vector",
        "baseline_bm25",
        "hybrid_rrf",
        "final_rrf_rerank"
    ]
    
    results_data = {}

    # 2. ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ 'ë‹µë³€' ë° 'ë¬¸ë§¥' ìƒì„± (LLM í˜¸ì¶œ)
    for model_name in models_to_test:
        print(f"\n--- [{model_name}] ëª¨ë¸ì˜ ê²°ê³¼ ìƒì„± ì¤‘ ---")
        model_results = []
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        for q in tqdm(questions, desc=f"Processing {model_name}"):
            model_results.append(get_rag_results(q, model_name))
        results_data[model_name] = model_results

    # 3. ê° ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ RAGAsë¡œ í‰ê°€
    final_scores = []
    
    for model_name, results_list in results_data.items():
        print(f"\n--- [{model_name}] ëª¨ë¸ì˜ RAGAs ì ìˆ˜ ê³„ì‚° ì¤‘ ---")
        
        # RAGAsê°€ ìš”êµ¬í•˜ëŠ” Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        eval_dataset = Dataset.from_list(results_list)
        
        # â˜… 2. context_relevancyë¥¼ metrics ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œì™¸
        result = evaluate(
            eval_dataset,
            metrics=[
                faithfulness,
                answer_relevancy
            ],
            llm=llm
        )

        # === (ì‹ ê·œ) GPT ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ì¶”ê°€ ===
                # === (ì‹ ê·œ) GPT ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ì¶”ê°€ ===
        print(f"--- {model_name} GPT Hallucination í‰ê°€ ì¤‘ ---")
        hallucination_scores = []
        coverage_scores = []
        consistency_scores = []
        fluency_scores = []

        hallucination_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert evaluator of factual accuracy for AI-generated answers."),
            ("user", """You are given:
                [Question]: {question}
                [Answer]: {answer}
                [Context]: {context}

                Does the answer contain any hallucination or information that is **not supported** by the context?
                Respond ONLY with a number between 0 and 1:
                - 0 = Fully factual (no hallucination)
                - 1 = Completely hallucinated (entirely unsupported)
                If partially hallucinated, respond with an intermediate value like 0.3 or 0.6.
                Output only the number.""")
            ])
        hallucination_chain = hallucination_prompt | llm | StrOutputParser()

        # ğŸ”½ ì—¬ê¸°ì— ì¶”ê°€
        coverage_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert evaluator of how well an answer uses context evidence."),
            ("user", """Question: {question}
                Answer: {answer}
                Context: {context}

                Rate how effectively the answer uses key information from the context.
                Respond ONLY with a number 0â€“1 (0 = not at all, 1 = fully uses context).""")
            ])
        coverage_chain = coverage_prompt | llm | StrOutputParser()

        consistency_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert evaluator of logical and factual consistency."),
            ("user", """Question: {question}
                Answer: {answer}
                Context: {context}

                Does the answer contradict itself or the provided context?
                0 = inconsistent, 1 = fully consistent.
                Respond ONLY with a number between 0 and 1.""")
            ])
        consistency_chain = consistency_prompt | llm | StrOutputParser()

        fluency_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert evaluating the fluency and readability of text."),
            ("user", """Answer: {answer}

                Rate the fluency, grammar, and clarity of this answer.
                Respond ONLY with a number between 0 and 1 (0 = poor, 1 = excellent).""")
            ])
        fluency_chain = fluency_prompt | llm | StrOutputParser()

        # === í‰ê°€ ë£¨í”„ ===
        for item in tqdm(results_list, desc=f"Hallucination Eval for {model_name}"):
            try:
                score_str = hallucination_chain.invoke({
                    "question": item["question"],
                    "answer": item["answer"],
                    "context": "\n".join(item["contexts"])
                }).strip()
                score = float(re.findall(r"\d*\.?\d+", score_str)[0])
            except Exception:
                score = 0.5
            hallucination_scores.append(score)

            # --- ì¶”ê°€ëœ 3ê°€ì§€ ì§€í‘œ ---
            try:
                coverage_str = coverage_chain.invoke({
                    "question": item["question"],
                    "answer": item["answer"],
                    "context": "\n".join(item["contexts"])
                }).strip()
                coverage_score = float(re.findall(r"\d*\.?\d+", coverage_str)[0])
            except Exception:
                coverage_score = 0.5
            coverage_scores.append(coverage_score)

            try:
                consistency_str = consistency_chain.invoke({
                    "question": item["question"],
                    "answer": item["answer"],
                    "context": "\n".join(item["contexts"])
                }).strip()
                consistency_score = float(re.findall(r"\d*\.?\d+", consistency_str)[0])
            except Exception:
                consistency_score = 0.5
            consistency_scores.append(consistency_score)

            try:
                fluency_str = fluency_chain.invoke({
                    "answer": item["answer"]
                }).strip()
                fluency_score = float(re.findall(r"\d*\.?\d+", fluency_str)[0])
            except Exception:
                fluency_score = 0.5
            fluency_scores.append(fluency_score)

        # === í‰ê·  ê³„ì‚° í›„ ì €ì¥ ===
        mean_hallucination = np.mean(hallucination_scores)
        mean_coverage = np.mean(coverage_scores)
        mean_consistency = np.mean(consistency_scores)
        mean_fluency = np.mean(fluency_scores)

        
        result_df = result.to_pandas()
        print(f"--- {model_name} ê°œë³„ ê²°ê³¼ ---")
        print(result_df)
        
        # â˜… 3. context_relevancyë¥¼ ê²°ê³¼ ì €ì¥ì—ì„œ ì œì™¸
        final_scores.append({
            "Model": model_name,
            "Faithfulness": result["faithfulness"],
            "Answer Relevancy": result["answer_relevancy"],
            "GPT_Hallucination": mean_hallucination,
            "Context_Coverage": mean_coverage,
            "Consistency": mean_consistency,
            "Fluency": mean_fluency
        })

    # 4. ìµœì¢… ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
    summary_df = pd.DataFrame(final_scores).set_index("Model")
    print("\n\n--- ğŸ“Š ìµœì¢… RAGAs í‰ê°€ ìš”ì•½ (ë…¼ë¬¸ í‘œ 1) ---")
    print(summary_df)
    

    from datetime import datetime

    # í˜„ì¬ ì‹œê°„ ë¬¸ìì—´ ìƒì„± (ì˜ˆ: 2025-11-14_03-25-10)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"ragas_evaluation_summary5_{timestamp}.csv"
    # 5. íŒŒì¼ë¡œ ì €ì¥
    #summary_df.to_csv("ragas_evaluation_summary.csv")
    summary_df.to_csv(filename, index=False)
    print("ê²°ê³¼ë¥¼ ragas_evaluation_summary.csv íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# --- 3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    # API í‚¤ê°€ .env íŒŒì¼ì— ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ ìˆ˜ë™ ì„¤ì •
    # os.environ["OPENAI_API_KEY"] = "sk-..."
    
    run_evaluation()