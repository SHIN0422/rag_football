import json
import os
import re
import pickle
import math
from collections import defaultdict
from pathlib import Path
from datetime import datetime, date, timedelta
import numpy as np
import pandas as pd
from tqdm import tqdm

# --- LangChain ë° ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

try:
    from langchain_community.retrievers import BM25Retriever
except Exception:
    from langchain.retrievers import BM25Retriever

try:
    from sentence_transformers import CrossEncoder
except Exception:
    CrossEncoder = None
    print("[Warning] sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Rerankerê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

import openai

# --- ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
load_dotenv()

# LLM & ì„ë² ë”©
llm = ChatOpenAI(model="gpt-4o-mini")
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
persist_directory = str(BASE_DIR / "news_chroma_db")
bm25_index_path = BASE_DIR / "bm25_index.pkl"

# ì„¤ì • íŒŒì¼ ë¡œë“œ (config.json)
try:
    with open(BASE_DIR / "config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
except Exception:
    config = {}

# íŒ€ ì´ë¦„ ë³€í™˜ ì‚¬ì „
translation_dict = config.get("translation_dict", {})

# ì „ì—­ ë¦¬íŠ¸ë¦¬ë²„ ë° ë­ì»¤ ë³€ìˆ˜
vector_retriever = None
bm25_global = None
bm25_doc_count = 0
bm25_all_docs = []
reranker = None
parser = StrOutputParser()


# --- 1. rag.pyì˜ í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (ì›ë³¸ ìœ ì§€) ---
# (translate_query, rrf_fuse, gpt_translate_korean_to_english, 
#  build_global_bm25, init_reranker_from_config, rerank_with_cross_encoder,
#  _detect_news_category, _build_chain_for_news, create_rag_chain)
# (ì´ì „ ì½”ë“œì™€ ë™ì¼í•œ í•¨ìˆ˜ë“¤ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµí•©ë‹ˆë‹¤)

def translate_query(query: str, dictionary: dict) -> str:
    if not query or not dictionary:
        return query
    for kor, eng in dictionary.items():
        query = query.replace(kor, eng)
    return query

def rrf_fuse(result_lists, k=36, C=60):
    scores, pick = defaultdict(float), {}
    for results in result_lists: 
        for rank, d in enumerate(results):
            key = d.page_content
            scores[key] += 1.0 / (C + rank + 1)
            pick.setdefault(key, d)
    merged = [pick[key] for key, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)]
    return merged[:k]

def gpt_translate_korean_to_english(query: str, model="gpt-4o-mini") -> str:
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Translate the following Korean football question into English for use in a document search engine. Be concise."),
        ("human", "{q}")
    ])
    chain = prompt | ChatOpenAI(model=model, temperature=0) | StrOutputParser()
    return chain.invoke({"q": query})

def build_global_bm25():
    global bm25_global, bm25_doc_count, bm25_all_docs
    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=hf_embeddings,
        collection_name="news_collection",
    )
    try:
        all_db_ids = set(db.get(include=[])['ids'])
    except Exception:
        all_db_ids = set()
    old_docs = []
    processed_ids = set()
    if bm25_index_path.exists():
        try:
            with open(bm25_index_path, "rb") as f:
                saved_data = pickle.load(f)
                old_docs = saved_data.get('docs', [])
                processed_ids = saved_data.get('ids', {d.metadata.get('id') for d in old_docs})
            print(f"[bm25] {len(processed_ids)}ê°œì˜ ê¸°ì¡´ ë¬¸ì„œ ì •ë³´ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception:
            print("[bm25] ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            old_docs = []
            processed_ids = set()
    new_doc_ids = list(all_db_ids - processed_ids)
    if not new_doc_ids:
        print("[bm25] ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        if bm25_global is None and old_docs:
            bm25_global = BM25Retriever.from_documents(old_docs)
            bm25_global.k = int(config.get("bm25_k", 20))
            bm25_all_docs = old_docs
            bm25_doc_count = len(old_docs)
        return len(processed_ids)
    print(f"[bm25] {len(new_doc_ids)}ê°œì˜ ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    new_docs_data = db.get(ids=new_doc_ids, include=["documents", "metadatas"])
    new_docs = [
        Document(page_content=c, metadata=m)
        for c, m in zip(new_docs_data.get("documents", []), new_docs_data.get("metadatas", []))
        if c
    ]
    final_docs = old_docs + new_docs
    print(f"[bm25] ì´ {len(final_docs)}ê°œ ë¬¸ì„œë¡œ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
    bm25 = BM25Retriever.from_documents(final_docs)
    bm25.k = int(config.get("bm25_k", 20))
    bm25_global = bm25
    bm25_all_docs = final_docs
    bm25_doc_count = len(final_docs)
    try:
        with open(bm25_index_path, "wb") as f:
            pickle.dump({'docs': final_docs, 'ids': all_db_ids}, f)
        print(f"[bm25] ìµœì‹  ì¸ë±ìŠ¤ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {bm25_index_path}")
    except Exception as e:
        print(f"[bm25] ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    return bm25_doc_count

def init_reranker_from_config(cfg: dict):
    global reranker
    if not cfg.get("use_reranker", True):
        return
    if CrossEncoder is None:
        print("[reranker] sentence-transformers ë¯¸ì„¤ì¹˜ â†’ ê±´ë„ˆëœ€")
        return
    model = cfg.get("reranker_model", "BAAI/bge-reranker-base")
    max_len = int(cfg.get("reranker_max_length", 512))
    try:
        reranker = CrossEncoder(model, max_length=max_len)
        print(f"[reranker] loaded: {model}")
    except Exception as e:
        reranker = None
        print(f"[reranker] load failed: {e}")

def rerank_with_cross_encoder(query: str, docs, top_n=12, batch_size=16):
    if not docs:
        return []
    if reranker is None:
        return docs[:top_n]
    pairs = [[query, d.page_content] for d in docs]
    try:
        scores = reranker.predict(pairs, batch_size=batch_size, show_progress_bar=False)
    except Exception:
        return docs[:top_n]
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, _ in ranked[:top_n]]

def _detect_news_category(q: str) -> str:
    prompt = f"""
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¶•êµ¬ 'ë‰´ìŠ¤' ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë‹¤ìŒ 6ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”.
        - transfer: ì´ì ì„¤, ì˜ì…, ë°©ì¶œ, ì¬ê³„ì•½ ê´€ë ¨ ì§ˆë¬¸
        - injury: ì„ ìˆ˜ì˜ ë¶€ìƒ, ì§•ê³„, ì»¨ë””ì…˜ ë¬¸ì œ ê´€ë ¨ ì§ˆë¬¸
        - preview: ì•ìœ¼ë¡œ ì—´ë¦´ ê²½ê¸°ì— ëŒ€í•œ ì˜ˆì¸¡, ê´€ì „ í¬ì¸íŠ¸, ì˜ˆìƒ ë¼ì¸ì—… ê´€ë ¨ ì§ˆë¬¸
        - review: ì´ë¯¸ ëë‚œ ê²½ê¸°ì˜ ê²°ê³¼, í•˜ì´ë¼ì´íŠ¸, ë¶„ì„, ê²°ì •ì  ì¥ë©´ ê´€ë ¨ ì§ˆë¬¸
        - performance: íŠ¹ì • ì„ ìˆ˜ë‚˜ íŒ€ì˜ ìµœê·¼ í™œì•½ìƒ, í¼, ìŠ¤íƒ¯, í‰ê°€ ê´€ë ¨ ì§ˆë¬¸
        - general: ìœ„ì˜ 5ê°€ì§€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ëª¨ë“  ì¼ë°˜ì ì¸ ì •ë³´ ì§ˆë¬¸
        **ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì•„ë˜ 6ê°œì˜ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.**
        transfer / injury / preview / review / performance / general
        ì‚¬ìš©ì ì§ˆë¬¸: "{q}"
        ë¶„ë¥˜:
        """
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        result = response.choices[0].message.content.strip().lower()
        valid_categories = {"transfer", "injury", "preview", "review", "performance", "general"}
        return result if result in valid_categories else "general"
    except Exception as e:
        print(f"ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "general"

def _build_chain_for_news(category: str):
    system_message = ""
    if category == "transfer":
        system_message = (
            "ë‹¹ì‹ ì€ ì´ì  ì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, "
            "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ì´ì ì„¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤. "
            "íŠ¹íˆ **'ì„ ìˆ˜ ì´ë¦„', 'ê´€ë ¨ êµ¬ë‹¨', 'ì˜ˆìƒ ì´ì ë£Œ/ì¡°ê±´', 'ë£¨ë¨¸ì˜ ì¶œì²˜ë‚˜ ì‹ ë¢°ë„'**ì— ì´ˆì ì„ ë§ì¶° ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì„¸ìš”. "
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."
        )
    elif category == "injury":
        system_message = (
            "ë‹¹ì‹ ì€ êµ¬ë‹¨ì˜ ê³µì‹ ì˜ë£ŒíŒ€ì²˜ëŸ¼ ë³´ê³ í•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ë“¤ì„ ê·¼ê±°ë¡œ, "
            "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ì„ ìˆ˜ì˜ ìƒíƒœì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."
            "**'ì„ ìˆ˜ ì´ë¦„', 'ë¶€ìƒ ë¶€ìœ„ ë° ì‹¬ê°ë„', 'ì˜ˆìƒ ê²°ì¥ ê¸°ê°„ ë˜ëŠ” ë³µê·€ ì‹œì '**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
        )
    else: # general ë° ê¸°íƒ€
        system_message = (
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¶•êµ¬ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤. "
            "ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì°¾ì•„ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. "
            "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."
        )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "ì•„ë˜ëŠ” ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° í•„ìš”í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n---\n{context}\n---\n\nì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n{input}")
    ])
    return prompt | llm | parser

def create_rag_chain():
    global vector_retriever, bm25_global, reranker, config
    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=hf_embeddings,
        collection_name="news_collection",
    )
    if config.get("use_mmr", True):
        vector_retriever = db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": int(config.get("mmr_k", 20)),
                "fetch_k": int(config.get("mmr_fetch_k", 80)),
                "lambda_mult": float(config.get("mmr_lambda", 0.7)),
            },
        )
    else:
        vector_retriever = db.as_retriever(search_kwargs={"k": int(config.get("k", 20))})
    build_global_bm25()
    init_reranker_from_config(config)
    try:
        count = db._collection.count()
    except Exception:
        count = 0
    return f"ì¤€ë¹„ ì™„ë£Œ / DB ë¬¸ì„œ ìˆ˜: {count} / BM25 ë¬¸ì„œ ìˆ˜: {bm25_doc_count}"


# --- 2. RAGAs í‰ê°€ ë¡œì§ (ìˆ˜ì •ë¨) ---

def load_testset(filepath="ragas_dataset.jsonl"):
    questions = []
    dataset_path = BASE_DIR / filepath
    if not dataset_path.exists():
        print(f"ì˜¤ë¥˜: {dataset_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    with open(dataset_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                if "reason" not in data and data.get("question"):
                    questions.append(data["question"])
            except json.JSONDecodeError:
                continue
    unique_questions = list(set(questions))
    print(f"'{filepath}'ì—ì„œ {len(unique_questions)}ê°œì˜ ê³ ìœ í•œ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    sample_size = min(len(unique_questions), 30)
    return unique_questions[:sample_size]

def get_rag_results(question: str, model_type: str):
    q_preprocessed = translate_query(question, translation_dict).lower()
    q_translated = gpt_translate_korean_to_english(q_preprocessed)
    vector_docs = vector_retriever.invoke(q_translated)
    bm_docs = bm25_global.invoke(q_translated) if bm25_global is not None else []
    final_docs = []
    TOP_K = int(config.get("rrf_k", 10))
    if model_type == "baseline_vector":
        final_docs = vector_docs[:TOP_K]
    elif model_type == "baseline_bm25":
        final_docs = bm_docs[:TOP_K]
    elif model_type == "hybrid_rrf":
        candidates = rrf_fuse(
            [vector_docs, bm_docs],
            k=int(config.get("rrf_candidates_k", 20)),
            C=int(config.get("rrf_C", 60)),
        )
        final_docs = candidates[:TOP_K]
    elif model_type == "final_rrf_rerank":
        candidates = rrf_fuse(
            [vector_docs, bm_docs],
            k=int(config.get("rrf_candidates_k", 20)),
            C=int(config.get("rrf_C", 60)),
        )
        final_docs = rerank_with_cross_encoder(
            question, 
            candidates,
            top_n=TOP_K,
            batch_size=int(config.get("reranker_batch_size", 16)),
        )
    context_str = "\n\n".join(d.page_content for d in final_docs)
    category = _detect_news_category(question)
    rag_chain = _build_chain_for_news(category)
    try:
        answer = rag_chain.invoke({"context": context_str, "input": question})
    except Exception as e:
        print(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜ (ì§ˆë¬¸: {question[:20]}...): {e}")
        answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    contexts = [d.page_content for d in final_docs]
    return {
        "question": question,
        "answer": answer,
        "contexts": contexts
    }

def run_evaluation_custom_relevancy_only():
    """
    (â˜… ìˆ˜ì •) 4ê°œ ëª¨ë¸ í‰ê°€ë¥¼ 10íšŒ ë°˜ë³µí•˜ê³ ,
    'ë§¤ ì‹¤í–‰ë§ˆë‹¤' ì¤‘ê°„ ìš”ì•½ íŒŒì¼('ëª¨ë¸', 'í‰ê· ì ìˆ˜')ë§Œ ì €ì¥í•˜ë©°
    '10íšŒ ì¢…ë£Œ í›„' ìµœì¢… í‰ê·  ìš”ì•½ íŒŒì¼ë„ 1ê°œ ì €ì¥
    """
    
    # 0. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (1íšŒ ì‹¤í–‰)
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    init_status = create_rag_chain()
    print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {init_status}")

    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (1íšŒ ì‹¤í–‰)
    questions = load_testset("ragas_dataset.jsonl")
    if not questions:
        print("í‰ê°€í•  ì§ˆë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    models_to_test = [
        "baseline_vector",
        "baseline_bm25",
        "hybrid_rrf",
        "final_rrf_rerank"
    ]
    
    # 3. GPT ê¸°ë°˜ 'Answer Relevancy' ì»¤ìŠ¤í…€ í‰ê°€ ì •ì˜ (1íšŒ ì‹¤í–‰)
    print("\n--- ì»¤ìŠ¤í…€ í‰ê°€ í”„ë¡¬í”„íŠ¸ ì •ì˜ ---")
    
    relevancy_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an expert evaluator. Rate the relevance of the generated answer to the given question."),
        ("user", """
        [Question]: {question}
        [Answer]: {answer}

        Rate how relevant the [Answer] is to the [Question].
        Respond ONLY with a number between 0 and 1:
        - 0 = Completely irrelevant, or a refusal like "I don't know".
        - 1 = Perfectly relevant and directly answers the question.
        
        Output only the number.
        """)
    ])
    
    relevancy_chain = relevancy_prompt | llm | StrOutputParser()
    
    # --- (â˜… ìˆ˜ì •ëœ í‰ê°€ ë¡œì§) ---

    # 4. (ì‹ ê·œ) 10íšŒ ì‹¤í–‰ ì ìˆ˜ë¥¼ ëˆ„ì í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™” (ìµœì¢… ìš”ì•½ìš©)
    all_run_scores = {model_name: [] for model_name in models_to_test}

    num_runs = 10
    print(f"ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ RAG í‰ê°€ë¥¼ {num_runs}íšŒ ë°˜ë³µí•©ë‹ˆë‹¤.")

    # 5. (ì‹ ê·œ) 10íšŒ ë°˜ë³µ ë£¨í”„ ì‹œì‘
    for i in range(1, num_runs + 1):
        print(f"\n--- [ {i} / {num_runs} ë²ˆì§¸ ì‹¤í–‰ ] ---")
        
        results_data = {}
        
        # (â˜… ì‹ ê·œ) 'ì´ë²ˆ íšŒì°¨'ì˜ ìš”ì•½ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        final_scores_THIS_RUN = []

        # 5-1. ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ 'ë‹µë³€' ìƒì„±
        for model_name in models_to_test:
            print(f"  [{model_name}] ëª¨ë¸ì˜ ê²°ê³¼ ìƒì„± ì¤‘ (ì‹¤í–‰ {i})")
            model_results = []
            for q in tqdm(questions, desc=f"  Processing {model_name} (Run {i})", leave=False):
                model_results.append(get_rag_results(q, model_name))
            results_data[model_name] = model_results

        # 5-2. ê° ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì»¤ìŠ¤í…€ í‰ê°€
        for model_name, results_list in results_data.items():
            print(f"  [{model_name}] ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ ê´€ë ¨ì„±(Relevancy) ì ìˆ˜ ê³„ì‚° ì¤‘ (ì‹¤í–‰ {i})")
            
            relevancy_scores = []

            for item in tqdm(results_list, desc=f"  Evaluating {model_name} (Run {i})", leave=False):
                question = item["question"]
                answer = item["answer"]
                score = 0.5 # ê¸°ë³¸ê°’

                try:
                    score_str = relevancy_chain.invoke({
                        "question": question,
                        "answer": answer
                    }).strip()
                    score = float(re.findall(r"\d*\.?\d+", score_str)[0])
                except Exception as e:
                    print(f"    ì»¤ìŠ¤í…€ í‰ê°€ ì˜¤ë¥˜ (ì§ˆë¬¸: {question[:10]}...): {e}")
                    score = 0.0 # í‰ê°€ ì‹¤íŒ¨ ì‹œ 0ì 
                
                relevancy_scores.append(score)
                
                # (â˜… ì‚­ì œ) 'ì´ë²ˆ íšŒì°¨' ìƒì„¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ ì œê±°
            
            # 5-3. (ì‹ ê·œ) 'ì´ë²ˆ íšŒì°¨' í‰ê·  ì ìˆ˜ ê³„ì‚° ë° 'ìµœì¢…' ìš”ì•½ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
            mean_relevancy_this_run = np.mean(relevancy_scores)
            all_run_scores[model_name].append(mean_relevancy_this_run) # ìµœì¢… ìš”ì•½ìš©
            final_scores_THIS_RUN.append({ # ì´ë²ˆ íšŒì°¨ ìš”ì•½ìš©
                "Model": model_name,
                "Custom_Relevancy": mean_relevancy_this_run
            })
            print(f"  [{model_name}] (ì‹¤í–‰ {i}) í‰ê·  ì ìˆ˜: {mean_relevancy_this_run:.4f}")
        
        # --- (â˜… ì‹ ê·œ) 1íšŒì°¨ ì‹¤í–‰ì´ ëë‚  ë•Œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ ---
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        
        # 1. (â˜… ì‚­ì œ) 'ì´ë²ˆ íšŒì°¨' ìƒì„¸ íŒŒì¼ ì €ì¥ ë¡œì§ ì œê±°
        
        # 2. 'ì´ë²ˆ íšŒì°¨' ìš”ì•½ íŒŒì¼ ì €ì¥ (ìš”ì²­ëŒ€ë¡œ 'ëª¨ë¸'ê³¼ 'ì ìˆ˜'ë§Œ í¬í•¨)
        summary_df = pd.DataFrame(final_scores_THIS_RUN).set_index("Model")
        summary_filename = f"RUN_{i}_custom_relevancy_SUMMARY_{timestamp}.csv"
        summary_df.to_csv(summary_filename)
        print(f"\nâ˜… [{i}íšŒì°¨] ìš”ì•½ë³¸ì„ {summary_filename} ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        # --- (â˜… ì¤‘ê°„ ì €ì¥ ë¡œì§ ë) ---

    # --- (â˜… 10íšŒ ë£¨í”„ ì¢…ë£Œ í›„ ìµœì¢… ì§‘ê³„) ---
    print("\n\n--- ğŸ“Š 10íšŒ ì‹¤í–‰ ìµœì¢… í‰ê·  ê³„ì‚° ---")
    
    final_average_scores = []
    
    # 6. 10íšŒ ì‹¤í–‰ì˜ ìµœì¢… í‰ê·  ê³„ì‚°
    for model_name, score_list in all_run_scores.items():
        final_average = np.mean(score_list)
        final_average_scores.append({
            "Model": model_name,
            "Average_of_10_Runs": final_average,
            "All_10_Scores": str(score_list) # ì°¸ê³ ìš©ìœ¼ë¡œ 10íšŒ ì ìˆ˜ ëª©ë¡ë„ í¬í•¨
        })

    summary_df = pd.DataFrame(final_average_scores).set_index("Model")
    
    print("\n--- ğŸ“Š 10íšŒ ì‹¤í–‰ ìµœì¢… í‰ê·  ìš”ì•½ ---")
    print(summary_df)
    
    # 7. (ì‹ ê·œ) 10íšŒ í‰ê·  ìš”ì•½ë³¸ì„ 'í•˜ë‚˜ì˜ íŒŒì¼'ë¡œ ì €ì¥
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    summary_filename = f"FINAL_10_RUN_AVERAGE_SUMMARY_{timestamp}.csv"
    
    summary_df.to_csv(summary_filename)
    print(f"\nâ˜… 10íšŒ ì‹¤í–‰ í‰ê·  ìš”ì•½ë³¸ì„ {summary_filename} ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print("ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    # --- (â˜… í‰ê°€ ë¡œì§ ì¢…ë£Œ) ---


# --- 3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    run_evaluation_custom_relevancy_only()