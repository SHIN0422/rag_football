import json
import os
import re
import pickle
import math
from collections import defaultdict
from pathlib import Path
from datetime import datetime, date, timedelta
import numpy as np
import pandas as pd
from datasets import Dataset
from ragas import evaluate
# RAGAs í‰ê°€ ì§€í‘œ
from ragas.metrics import (
    faithfulness,
    answer_relevancy
)
from tqdm import tqdm

# --- LangChain ë° ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

import openai

# --- ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
load_dotenv()

# LLM & ì„ë² ë”©
llm = ChatOpenAI(model="gpt-4o-mini")
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
persist_directory = str(BASE_DIR / "news_chroma_db")

# ì„¤ì • íŒŒì¼ ë¡œë“œ (config.json)
try:
    with open(BASE_DIR / "config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
except Exception:
    config = {}

# íŒ€ ì´ë¦„ ë³€í™˜ ì‚¬ì „
translation_dict = config.get("translation_dict", {})

# ì „ì—­ ë¦¬íŠ¸ë¦¬ë²„ ë° íŒŒì„œ ë³€ìˆ˜
vector_retriever = None
parser = StrOutputParser()

# --- 1. RAG í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (ì´ì „ê³¼ ë™ì¼) ---

def translate_query(query: str, dictionary: dict) -> str:
    """ê°„ë‹¨í•œ ë‹¨ì–´ ê²½ê³„ ì¹˜í™˜(ì‚¬ì „ ì—†ìœ¼ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ)"""
    if not query or not dictionary:
        return query
    for kor, eng in dictionary.items():
        query = query.replace(kor, eng)
    return query

def gpt_translate_korean_to_english(query: str, model="gpt-4o-mini") -> str:
    """GPTë¥¼ ì´ìš©í•œ í•œ->ì˜ ë²ˆì—­"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Translate the following Korean football question into English for use in a document search engine. Be concise."),
        ("human", "{q}")
    ])
    chain = prompt | ChatOpenAI(model=model, temperature=0) | StrOutputParser()
    return chain.invoke({"q": query})

def _build_general_chain():
    """
    AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì—†ì´, í•­ìƒ ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©í•˜ëŠ” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    system_message = (
        "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¶•êµ¬ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì°¾ì•„ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. "
        "í•­ìƒ ê°ê´€ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ì •ë³´ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤."
        "ì£¼ì–´ì§„ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë³¸ë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ë¡ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤."
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "ì•„ë˜ëŠ” ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° í•„ìš”í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n---\n{context}\n---\n\nì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n{input}")
    ])
    
    return prompt | llm | parser

def create_rag_chain_simple():
    """
    í‰ê°€ì— í•„ìš”í•œ ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    global vector_retriever, config

    # 1. ë²¡í„° DB ë¡œë“œ
    db = Chroma(
        persist_directory=persist_directory,
        embedding_function=hf_embeddings,
        collection_name="news_collection",
    )

    # 2. ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    if config.get("use_mmr", True):
        vector_retriever = db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": int(config.get("mmr_k", 20)),
                "fetch_k": int(config.get("mmr_fetch_k", 80)),
                "lambda_mult": float(config.get("mmr_lambda", 0.7)),
            },
        )
    else:
        vector_retriever = db.as_retriever(search_kwargs={"k": int(config.get("k", 20))})

    # 3. ìƒíƒœ ë©”ì‹œì§€ ë°˜í™˜
    try:
        count = db._collection.count()
    except Exception:
        count = 0
    return f"ì¤€ë¹„ ì™„ë£Œ / DB ë¬¸ì„œ ìˆ˜: {count} (Vector Retrieverë§Œ í™œì„±í™”)"


# --- 2. RAGAs í‰ê°€ ë¡œì§ (ìˆ˜ì •/ë‹¨ìˆœí™”) ---

def load_testset(filepath="ragas_dataset.jsonl"):
    """'satisfied' í”¼ë“œë°±('reason'ì´ ì—†ëŠ”) ì§ˆë¬¸ë§Œ ë¡œë“œ"""
    questions = []
    
    dataset_path = BASE_DIR / filepath
    if not dataset_path.exists():
        print(f"ì˜¤ë¥˜: {dataset_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    with open(dataset_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                if "reason" not in data and data.get("question"):
                    questions.append(data["question"])
            except json.JSONDecodeError:
                continue
    
    unique_questions = list(set(questions))
    print(f"'{filepath}'ì—ì„œ {len(unique_questions)}ê°œì˜ ê³ ìœ í•œ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    sample_size = min(len(unique_questions), 30)
    return unique_questions[:sample_size]

def get_rag_results_simple(question: str):
    """
    ì˜¤ì§ 'ë²¡í„° ê²€ìƒ‰ + ì¼ë°˜ í”„ë¡¬í”„íŠ¸' RAGë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    # 1. ì§ˆì˜ ì „ì²˜ë¦¬
    q_preprocessed = translate_query(question, translation_dict).lower()
    q_translated = gpt_translate_korean_to_english(q_preprocessed)
    
    # 2. ë¦¬íŠ¸ë¦¬ë²Œ (Vectorë§Œ)
    vector_docs = vector_retriever.invoke(q_translated)
    
    # 3. í›„ë³´êµ° í™•ì •
    TOP_K = int(config.get("rrf_k", 10)) 
    final_docs = vector_docs[:TOP_K]

    # 4. ë‹µë³€ ìƒì„± (ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ì²´ì¸ ì‚¬ìš©)
    context_str = "\n\n".join(d.page_content for d in final_docs)
    
    rag_chain = _build_general_chain()
    
    try:
        answer = rag_chain.invoke({"context": context_str, "input": question})
    except Exception as e:
        print(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜ (ì§ˆë¬¸: {question[:20]}...): {e}")
        answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    contexts = [d.page_content for d in final_docs]
    
    return {
        "question": question,
        "answer": answer,
        "contexts": contexts
    }

def run_evaluation_simple():
    """(ìˆ˜ì •) 'ë²¡í„° ê²€ìƒ‰ + ì¼ë°˜ í”„ë¡¬í”„íŠ¸' ëª¨ë¸ë§Œ RAGAsë¡œ í‰ê°€"""
    
    # 0. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Vectorë§Œ)
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    init_status = create_rag_chain_simple()
    print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {init_status}")

    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    questions = load_testset("ragas_dataset.jsonl")
    if not questions:
        print("í‰ê°€í•  ì§ˆë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. RAGAS_DATASET_FILEì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ RAGAs í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    model_name = "simple_vector_general_prompt"
    
    # 2. ëª¨ë¸ ê²°ê³¼ ìƒì„± (LLM í˜¸ì¶œ)
    print(f"\n--- [{model_name}] ëª¨ë¸ì˜ ê²°ê³¼ ìƒì„± ì¤‘ ---")
    model_results = []
    for q in tqdm(questions, desc=f"Processing {model_name}"):
        model_results.append(get_rag_results_simple(q))

    # 3. RAGAsë¡œ í‰ê°€
    print(f"\n--- [{model_name}] ëª¨ë¸ì˜ RAGAs ì ìˆ˜ ê³„ì‚° ì¤‘ ---")
    
    eval_dataset = Dataset.from_list(model_results)
    
    result = evaluate(
        eval_dataset,
        metrics=[
            faithfulness,
            answer_relevancy
        ],
        llm=llm
    )

    # --- (â˜… ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘) ---
    
    # 4. ê°œë³„ ìƒì„¸ ê²°ê³¼ ì¶œë ¥
    # result.to_pandas()ëŠ” 'question', 'answer', 'contexts', 'faithfulness', 'answer_relevancy' ë“±ì„ í¬í•¨
    result_df = result.to_pandas()
    print(f"\n\n--- ğŸ“Š [{model_name}] ê°œë³„ ìƒì„¸ ê²°ê³¼ ---")
    
    # í„°ë¯¸ë„ì— ì¶œë ¥í•  ë•Œ 'contexts' ì—´ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸í•˜ê³ , ì£¼ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ
    display_columns = ['question', 'answer', 'faithfulness', 'answer_relevancy']
    
    # RAGAs ë²„ì „ì— ë”°ë¼ 'contexts'ê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ í™•ì¸
    all_display_columns = [col for col in display_columns if col in result_df.columns]
    
    # ë³´ê¸° ì¢‹ê²Œ ì£¼ìš” ì»¬ëŸ¼ë§Œ ì¶œë ¥
    with pd.option_context('display.max_rows', None, 'display.max_colwidth', 60):
        print(result_df[all_display_columns])
    
    # 5. í‰ê·  ì ìˆ˜ë„ ë³„ë„ë¡œ ì¶œë ¥
    print("\n\n--- ğŸ“Š RAGAs í‰ê°€ ìš”ì•½ (í‰ê· ) ---")
    print(f"Model: {model_name}")
    print(f"Faithfulness (Avg): {result['faithfulness']}")
    print(f"Answer Relevancy (Avg): {result['answer_relevancy']}")

    # 6. (ìˆ˜ì •) íŒŒì¼ë¡œ ì €ì¥ (í‰ê·  ìš”ì•½ì´ ì•„ë‹Œ, ê°œë³„ ìƒì„¸ ê²°ê³¼ë¥¼ ì €ì¥)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # íŒŒì¼ ì´ë¦„ ë³€ê²½ (DETAILED ëª…ì‹œ)
    filename = f"ragas_simple_vector_eval_DETAILED_{timestamp}.csv"
    
    # (ìˆ˜ì •) result_df (ê°œë³„ ê²°ê³¼)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    # CSV íŒŒì¼ì´ ì—‘ì…€ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ 'utf-8-sig' ì¸ì½”ë”© ì‚¬ìš©
    result_df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… ê°œë³„ ìƒì„¸ ê²°ê³¼({len(result_df)}ê°œ í•­ëª©)ë¥¼ {filename} íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print("ì—‘ì…€ì—ì„œ ì´ íŒŒì¼ì„ ì—´ì–´ 'answer_relevancy'ê°€ 0.0ì¸ í•­ëª©ì˜ 'answer'ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.")
    # --- (â˜… ìˆ˜ì •ëœ ë¶€ë¶„ ë) ---


# --- 3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    run_evaluation_simple()